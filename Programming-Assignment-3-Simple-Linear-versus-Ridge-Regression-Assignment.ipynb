{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 3 - Simple Linear versus Ridge Regression (70 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your friend Bob just moved to Boston. He is a real estate agent who is trying to evaluate the prices of houses in the Boston area. He has been using a linear regression model but it has been taking a long time to compute prices for new hourse. He comes to you for help as he knows that you're an expert in machine learning. As a pro, you suggest ridge regression to reduce the feature weights as a way to increase the speed of computation. Bob, however, being a skeptic isn't convinced. He wants you to write a program that illustrates the difference in training and test costs for both linear and ridge regression on the same dataset. Being a good friend, you oblige and hence this assignment :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Import the boston dataset from sklearn - 5 points. \n",
    "boston_data = sklearn.datasets.load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Create X and Y variables - X holding the .data and Y holding .target - 5 points\n",
    "X = boston_data.data\n",
    "Y = boston_data.target\n",
    "\n",
    "# TODO - Reshape Y to be a rank 2 matrix - 5 points\n",
    "Y = Y.reshape(Y.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scaling and centering of data\n",
    "X = sklearn.preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create polynomial feeatures for the dataset to test linear and ridge regression on data with d = 1 and data with d = 2. Feel free to increase the # of degress and see what effect it has on the training and test error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Create a PolynomialFeatures object with degree = 2. \n",
    "# Transform X and save it into X_2. Simply copy Y into Y_2 - 10 points\n",
    "poly = PolynomialFeatures(degree = 2)\n",
    "X_2 = poly.fit_transform(X)\n",
    "Y_2 = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 105)\n",
      "(506, 1)\n"
     ]
    }
   ],
   "source": [
    "# CHECK - the shape of X_2 and Y_2 - should be (506, 105) and (506, 1) respectively\n",
    "print(X_2.shape)\n",
    "print(Y_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split both the sets of data into training and testing data to get a more accurate picture of real-world performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 13) (152, 13) (354, 1) (152, 1)\n",
      "(354, 105) (152, 105) (354, 1) (152, 1)\n"
     ]
    }
   ],
   "source": [
    "# TODO - Call train_test_split for both X, Y and X_2, Y_2. Don't pass in any other parameters except for the dataset. \n",
    "# Let the split be the default value of 0.7 => 70% training and 30% test. - 10 points\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y)\n",
    "X_2_train, X_2_test, Y_2_train, Y_2_test = train_test_split(X_2,Y_2)\n",
    "\n",
    "# CHECK - Shapes of the resulting variables should be as follows:\n",
    "# (379, 13) (127, 13) (379, 1) (127, 1)\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "# (379, 105) (127, 105) (379, 1) (127, 1)\n",
    "print(X_2_train.shape, X_2_test.shape, Y_2_train.shape, Y_2_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression and Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Define the get_error_ridge function. Use Ridge() from sklearn.\n",
    "# TODO - Return train_error and test_error values\n",
    "# Some important functions to know are .fit() and .predict()\n",
    "# HINT - fit() the data on training data and predict() on training and testing data to get the train and test error\n",
    "# 15 points\n",
    "\n",
    "def get_error_ridge(X_train, X_test, Y_train, Y_test, alpha):\n",
    "    model = sklearn.linear_model.Ridge(alpha = alpha)\n",
    "    fit = model.fit(X_train,Y_train)\n",
    "    predictTrain = model.predict(X_train)\n",
    "    predictTest = model.predict(X_test)\n",
    "    \n",
    "    train_error =np.sum(np.square(Y_train - predictTrain)) / (Y_train.shape[0]) \n",
    "    test_error = np.sum(np.square(Y_test - predictTest)) / (Y_test.shape[0])\n",
    "    \n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For linear regression and no polynomial features: train_error = 18.46 and test_error = 31.45\n",
      "For linear regression and with polynomial features: train_error = 5.68 and test_error = 16.42\n"
     ]
    }
   ],
   "source": [
    "# Call the function above with alpha = 0 - Notice that when alpha = 0, ridge regression and linear regression are equivalent\n",
    "lin_rss = get_error_ridge(X_train, X_test, Y_train, Y_test, 0)\n",
    "lin_poly_rss = get_error_ridge(X_2_train, X_2_test, Y_2_train, Y_2_test, 0)\n",
    "\n",
    "print(\"For linear regression and no polynomial features: train_error = {0:.2f} and test_error = {1:.2f}\".format(lin_rss[0], lin_rss[1]))\n",
    "print(\"For linear regression and with polynomial features: train_error = {0:.2f} and test_error = {1:.2f}\".format(lin_poly_rss[0], lin_poly_rss[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ridge regression and no polynomial features: train_error = 18.46 and test_error = 31.44\n",
      "For ridge regression and with polynomial features: train_error = 5.83 and test_error = 14.45\n"
     ]
    }
   ],
   "source": [
    "# Call the function above with alpha = 1\n",
    "lin_rss = get_error_ridge(X_train, X_test, Y_train, Y_test,1)\n",
    "lin_poly_rss = get_error_ridge(X_2_train, X_2_test, Y_2_train, Y_2_test, 1 )\n",
    "\n",
    "print(\"For ridge regression and no polynomial features: train_error = {0:.2f} and test_error = {1:.2f}\".format(lin_rss[0], lin_rss[1]))\n",
    "print(\"For ridge regression and with polynomial features: train_error = {0:.2f} and test_error = {1:.2f}\".format(lin_poly_rss[0], lin_poly_rss[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Define the get_coeff_ridge_normaleq function. Use the normal equation method.\n",
    "# TODO - Return w values\n",
    "# 10 points\n",
    "\n",
    "def get_coeff_ridge_normaleq(X_train, Y_train, alpha):\n",
    "    # a = X.T.(X) + alpha * np.identity(# of columns in X)\n",
    "    a = np.transpose(X_train).dot(X_train) + alpha * np.identity(X_train.shape[1])\n",
    "    # q is the pseudo inverse of a\n",
    "    q = np.linalg.pinv(a)\n",
    "    # z is X.T.dot(Y)\n",
    "    z = np.transpose(X_train).dot(Y_train)\n",
    "    # w = q dot z - reshape to (# columns in X, 1)\n",
    "    w = np.reshape(q.dot(z),(X_train.shape[1],1))\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Define the evaluate_err_ridge_normaleq function.\n",
    "# TODO - Return the train_error and test_error values\n",
    "# 10 points\n",
    "\n",
    "def evaluate_err_ridge_normaleq(X_train, X_test, Y_train, Y_test, w): \n",
    "    yhat = X_train.dot(w)\n",
    "    yhatTest = X_test.dot(w)\n",
    "    \n",
    "    train_error = np.sum(np.square(Y_train - yhat)) / (Y_train.shape[0]) \n",
    "    test_error = np.sum(np.square(Y_test - yhatTest)) / (Y_test.shape[0])\n",
    "    \n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For linear regression and no polynomial features: train_error = 512.28 and test_error = 588.89\n",
      "For linear regression and with polynomial features: train_error = 5.67 and test_error = 16.40\n"
     ]
    }
   ],
   "source": [
    "# Call the function above with alpha = 0 - Notice that when alpha = 0, ridge regression and linear regression are equivalent\n",
    "w = get_coeff_ridge_normaleq(X_train, Y_train, 0)\n",
    "lin_rss = evaluate_err_ridge_normaleq(X_train, X_test, Y_train, Y_test, w)\n",
    "\n",
    "w_2 = get_coeff_ridge_normaleq(X_2_train, Y_2_train, 0)\n",
    "lin_poly_rss = evaluate_err_ridge_normaleq(X_2_train, X_2_test, Y_2_train, Y_2_test, w_2)\n",
    "\n",
    "print(\"For linear regression and no polynomial features: train_error = {0:.2f} and test_error = {1:.2f}\".format(lin_rss[0], lin_rss[1]))\n",
    "print(\"For linear regression and with polynomial features: train_error = {0:.2f} and test_error = {1:.2f}\".format(lin_poly_rss[0], lin_poly_rss[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ridge regression and no polynomial features: train_error = 512.29 and test_error = 588.82\n",
      "For ridge regression and with polynomial features: train_error = 5.94 and test_error = 14.27\n"
     ]
    }
   ],
   "source": [
    "# Call the function above with alpha = 1\n",
    "w = get_coeff_ridge_normaleq(X_train, Y_train, 1)\n",
    "lin_rss = evaluate_err_ridge_normaleq(X_train, X_test, Y_train, Y_test, w)\n",
    "\n",
    "w_2 = get_coeff_ridge_normaleq(X_2_train, Y_2_train, 1)\n",
    "lin_poly_rss = evaluate_err_ridge_normaleq(X_2_train, X_2_test, Y_2_train, Y_2_test, w_2)\n",
    "\n",
    "print(\"For ridge regression and no polynomial features: train_error = {0:.2f} and test_error = {1:.2f}\".format(lin_rss[0], lin_rss[1]))\n",
    "print(\"For ridge regression and with polynomial features: train_error = {0:.2f} and test_error = {1:.2f}\".format(lin_poly_rss[0], lin_poly_rss[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
